---
title: "Tidymodels II: K-Fold Cross Validation"
subtitle: "Learn how to build models with tidymodels, Part 2"
author: "Prof. Dr. Jan Kirenz"
output:
 html_document:
  code_download: true 
  css: style.css 
  fig_height: 6
  fig_width: 8
  highlight: tango
  number_sections: yes
  theme: paper
  toc: yes
  toc_depth: 3
  toc_float: 
    collapsed: false
    smooth_scroll: true 
  df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidyverse)
library(tidymodels)
```

*This tutorial is based on Alisson Hill's excellent [tidymodels workshop](https://alison.rbind.io/tags/tidymodels/).* 

# Import data

```{r}
library(tidyverse)

ames <- read_csv("https://raw.githubusercontent.com/kirenz/datasets/master/ames.csv")

ames <- 
  ames %>%
  select(Sale_Price, Gr_Liv_Area)

glimpse(ames)

```


# Build a model

## Model specification

Build the model:

```{r}
library(tidymodels)

lm_spec <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode(mode = "regression")

```

## Data split

```{r}
set.seed(100)

new_split <- initial_split(ames) 
new_train <- training(new_split) 
new_test <- testing(new_split)

```

# Fit the model

```{r}

lm_fit <- 
  lm_spec %>%
  fit(Sale_Price ~ Gr_Liv_Area , data = new_train)

```

Make predictions and calculate the RMSE for our training data. 

```{r}

lm_fit %>% 
 predict(new_train) %>%
 mutate(truth = new_train$Sale_Price) %>%
 rmse(truth, .pred)

```


# Evaluate model

To further evaluate our simple Linear Regression model, let's build a validation set. Note that this process is usually used to compare the performance of multiple models and to tune hyperparameters. In this tutorial however, we only want to demonstrate the process and therefore only use one model.   

We can build a set of k-Fold cross validation sets and use this set of resamples to estimate the performance of our model using the `fit_resamples()` function. Note that this function does not do any tuning of the model parameters. The function is only used for computing performance metrics across some set of resamples. It will fit our model `lm_spec` to each resample and evaluate on the heldout set from each resample. It is important to note, that the function does not even keep the models it trains. 

First of all, we build a set of 10 validation folds with the function `vfold_cv` (we also use stratified sampling in this example):

```{r}

set.seed(100)

cv_folds <-
 vfold_cv(new_train, 
          v = 10, 
          strata = Sale_Price,
          breaks = 4) 

cv_folds

```

Next, let's fit the model on each of the 10 folds with `fit_resamples()` and store the results as `lm_res`:

```{r}

lm_res <-
  lm_spec %>% 
  fit_resamples(
    Sale_Price ~ Gr_Liv_Area,
    resamples = cv_folds
    )

```

Now we can collect the performance metrics with `collect_metrics()`. The metrics show the average performance across all folds.

```{r}

lm_res %>% 
  collect_metrics()

```

If we would be interested in the results of every split, we could use the option `summarize = FALSE`:

```{r}
lm_res %>% 
  collect_metrics(summarize = FALSE)
```


# Evaluate final model

Finally, let's use our testing data and see how we can expect this model to perform on new data.

```{r}

lm_fit %>% 
 predict(new_test) %>%
 mutate(truth = new_test$Sale_Price) %>%
 rmse(truth, .pred)

```


