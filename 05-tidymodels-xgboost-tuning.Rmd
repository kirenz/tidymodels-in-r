---
title: "Tidymodels V: Tune Models (XGBoost)"
subtitle: "Learn how to tune models with tidymodels, Part 5"
author: "Prof. Dr. Jan Kirenz"
output:
 html_document:
  code_download: true 
  css: style.css 
  fig_height: 6
  fig_width: 8
  highlight: tango
  number_sections: yes
  theme: paper
  toc: yes
  toc_depth: 3
  toc_float: 
    collapsed: false
    smooth_scroll: true 
  df_print: paged
---


```{r setup, include=TRUE}
library(knitr)

knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, fig.width = 8, fig.height = 5)
library(scales)
library(tidyverse)
library(tidymodels)
library(vip)

```


*This tutorial is based on Julia Silge's excellent [demonstration](https://www.r-bloggers.com/2020/05/tune-xgboost-with-tidymodels-and-tidytuesday-beach-volleyball/) on how to use the tidymodels framework to tune XGBoost and the [tidymodels documentation](https://www.tidymodels.org/start/recipes/).*


In this tutorial, we will tune a XGBoost model using flight data to predict whether a plane arrives more than 30 minutes late. The data set contains information on flights departing near New York City in 2013. Furthermore, it contains weather data (hourly meterological data for LGA, JFK and EWR).

Letâ€™s start by loading the data.

```{r}

link <- 'https://raw.githubusercontent.com/kirenz/tidymodels-in-r/main/df_flight_prep.csv'

flight_data <- read_csv(link)

glimpse(flight_data)

flight_data$arr_delay <- as.factor(flight_data$arr_delay)

```


# Data splitting

We can start by splitting our data into training and testing sets.

```{r}
library(tidymodels)
set.seed(123)

flight_split <- initial_split(flight_data)
flight_train <- training(flight_split)
flight_test <- testing(flight_split)

```

# Model building

We want to build a XGBoost model and tune the model hyperparameters. To get an overview of the hyperparamters, use the following funtion in R:

```{r eval=FALSE}

?boost_tree

```

## Specify model

Note that due to performance reasons, we set the size of some tuning parameters to relatively low numbers.

```{r}


xgb_spec <- 
  boost_tree(
  trees = 10, # usually we would set this to a higher value, like 1000
  tree_depth = tune(), 
  min_n = tune(), 
  loss_reduction = tune(), 
  sample_size = tune(), 
  mtry = tune(), 
  learn_rate = tune(),    
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_spec

```


* mtry: The number of predictors that will be randomly sampled at each split when creating the tree models.

* trees: The number of trees contained in the ensemble.

* min_n: The minimum number of data points in a node that are required for the node to be split further.

* tree_depth: The maximum depth of the tree (i.e. number of splits).

* learn_rate: The rate at which the boosting algorithm adapts from iteration-to-iteration.

* loss_reduction: The reduction in the loss function required to split further.

* sample_size: The amount of data exposed to the fitting routine.

* stop_iter: The number of iterations without improvement before stopping.

## Grid search

Let's set up possible values for these hyperparameters to try. We could use a [regular grid search](https://dials.tidymodels.org/reference/grid_regular.html) but that would take a long time. Instead, we use a space filling design called [`grid_latin_hypercube()`](https://dials.tidymodels.org/reference/grid_max_entropy.html), which is more efficient. It uses random sampling of points in the parameter space.

Due to performance reasons we set the size to a low number (20). In a real world scenario, we would probably use 50 to 100.

```{r}

xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(), # needs to be a proportion
  finalize(mtry(), flight_train), 
  learn_rate(),
  size = 20 # usually we would set this to a higher number
)

xgb_grid

```

Notice that we had to treat `mtry()` differently because it depends on the actual number of predictors in the data.

```{r}
mtry() # biggest possible number of mtry is unknown
```

## Model workflow

Let's put the model specification into a workflow for convenience. Since we don't have any complicated data preprocessing, we can use `add_formula()` as our data preprocessor.

```{r}

xgb_wf <- workflow() %>%
  add_formula(arr_delay ~ .) %>%
  add_model(xgb_spec)

xgb_wf

```

## Cross-validation

Next, let's create cross-validation resamples for tuning our model. Usually we would set "v" to 10 but due to performance reasons, we choose 5.


```{r}
set.seed(123)

vb_folds <- vfold_cv(flight_train, 
                     v = 5,
                     strata = arr_delay)

vb_folds

```

## Hyperparamater tuning

We use `tune_grid()` with our tuneable workflow, our resamples, and our grid of parameters to try. Let's use `control_grid(save_pred = TRUE)` so we can explore the predictions afterwards.

Note that this can take a while to process...

```{r}

set.seed(234)

xgb_res <- tune_grid(
  xgb_wf,
  resamples = vb_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE) # so we can make a ROC curve 
)

xgb_res

```



## Explore results

### Metrics

We can explore the metrics for all these models.

```{r}

collect_metrics(xgb_res)

```

### ROC

We can also use visualization to understand our results. Reshape the data to make a graph.

```{r}

xgb_res_long <-  
  xgb_res %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) 

xgb_res_long %>% 
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")

```



## Find best model


### Show best parameters

What are the best performing sets of parameters:

```{r}
show_best(xgb_res, 
          "roc_auc")
```


### Select best parameters

There may have been lots of parameters, but we were able to get good performance with several different combinations. Let's choose the best one.

```{r}

best_auc <- select_best(xgb_res, 
                        "roc_auc")
best_auc

```


## Finalize workflow

Now let's finalize our tuneable workflow with the best parameter values.

```{r}

final_xgb <- finalize_workflow(
  xgb_wf,
  best_auc
)

final_xgb

```

Instead of `tune()` placeholders, we now have real values for all the model hyperparameters.

### Fit final model

Fit the final model on the training data:

```{r}

final_fit_xgb <- 
  final_xgb %>%
  fit(data = flight_train)

```


To obtain information about the most important parameters in our model, we use a variable importance plot from the [vip package](https://koalaverse.github.io/vip/). 

```{r fig.width=7, fig.height=6}
library(vip)

final_fit_xgb %>%
  pull_workflow_fit() %>%
  vip(geom = "point") # variable importance plot

```

The predictors that are most important are shown at the top of the plot.


## Evaluate model 

It's time to go back to the testing set. Let's _evaluate_ our model one last time on the testing set. 


```{r}

final_res_pred <- 
  predict(final_fit_xgb, 
          flight_test, 
          type = "prob") %>% 
  mutate(pred_arr_delay = case_when(
            .pred_late >= 0.5 ~ "late",
            TRUE ~ "on_time"),
          pred_arr_delay = as.factor(pred_arr_delay)
          ) %>% 
  bind_cols(flight_test %>% 
              select(arr_delay, 
                     time_hour, 
                     flight)) 

```

### ROC

```{r, fig.width=6, fig.height=6}

final_res_pred %>%
   roc_curve(truth = arr_delay, 
            .pred_late) %>% 
  autoplot()
  
```


### AUC

`roc_auc()` estimates the area under the curve:

```{r}

final_res_pred %>% 
  roc_auc(truth = arr_delay, 
          .pred_late)

```


### Accuracy

We use the `metrics()` function to measure the performance of the model. It will automatically choose metrics appropriate for a given type of model. The function expects a tibble that contains the actual results (truth) and what the model predicted (estimate).

```{r}

final_res_pred %>%
  metrics(truth = arr_delay, 
          estimate = pred_arr_delay)

```


### Recall 

```{r}

final_res_pred %>%
  recall(truth = arr_delay, 
          estimate = pred_arr_delay)

```

### Precision

```{r}

final_res_pred %>%
  precision(truth = arr_delay, 
          estimate = pred_arr_delay)

```

