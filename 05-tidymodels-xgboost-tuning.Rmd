---
title: "Tidymodels V: Tune Models"
subtitle: "Learn how to build models with tidymodels, Part 5"
author: "Prof. Dr. Jan Kirenz"
output:
 html_document:
  code_download: true 
  css: style.css 
  fig_height: 6
  fig_width: 8
  highlight: tango
  number_sections: yes
  theme: paper
  toc: yes
  toc_depth: 3
  toc_float: 
    collapsed: false
    smooth_scroll: true 
  df_print: paged
---


```{r setup, include=TRUE}
library(knitr)

knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, fig.width = 8, fig.height = 5)
library(scales)
library(tidyverse)
library(tidymodels)
library(nycflights13)
library(vip)

```


*This tutorial is based on Julia Silge's excellent [demonstration](https://www.r-bloggers.com/2020/05/tune-xgboost-with-tidymodels-and-tidytuesday-beach-volleyball/) on how to use the tidymodels framework to tune XGBoost and the [tidymodels documentation](https://www.tidymodels.org/start/recipes/).*


Let's tune a XGBoost model using the `nycflights13` data to predict whether a plane arrives more than 30 minutes late. The data set contains information on flights departing near New York City in 2013. Furthermore, it contains weather data (hourly meterological data for LGA, JFK and EWR).

Letâ€™s start by loading the data.


# Data import

```{r}

link <- 'https://raw.githubusercontent.com/kirenz/tidymodels-in-r/main/df_flight_prep.csv'

flight_data <- read_csv(link)

flight_data

```


# Build a model

We can start by loading the tidymodels metapackage, and splitting our data into training and testing sets.

## Data splitting

```{r}
library(tidymodels)

set.seed(123)
flight_split <- initial_split(flight_data)

#flight_split$id <- tibble(id = "Resample1")

flight_train <- training(flight_split)
flight_test <- testing(flight_split)

```

## XGBoost hyperparamter

An XGBoost model is based on trees, so we don't need to do much preprocessing for our data; we don't need to worry about the factors or centering or scaling our data. Instead, we are going to tune model hyperparameters. Note that due to performance reasons, we set the size of some tuning parameters to relatively low numbers.


```{r}
# To get an overview of the hyperparamters:
# ?boost_tree

xgb_spec <- boost_tree(
  trees = 10, # usually we would set this to a higher value, like 1000
  tree_depth = tune(), # tune placeholder
  min_n = tune(), 
  loss_reduction = tune(),  ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),  ## randomness
  learn_rate = tune(),    ## step size
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_spec

```


* mtry: The number of predictors that will be randomly sampled at each split when creating the tree models.

* trees: The number of trees contained in the ensemble.

* min_n: The minimum number of data points in a node that are required for the node to be split further.

* tree_depth: The maximum depth of the tree (i.e. number of splits).

* learn_rate: The rate at which the boosting algorithm adapts from iteration-to-iteration.

* loss_reduction: The reduction in the loss function required to split further.

* sample_size: The amount of data exposed to the fitting routine.

* stop_iter: The number of iterations without improvement before stopping.

## Grid space

Let's set up possible values for these hyperparameters to try. 

* Option 1: Grid Search (would take a long time)

```{r}
# ?grid_regular
```

* Option 2: Space filling design (more efficient). 

```{r}
#?grid_latin_hypercube
```

We use a space-filling design so we can cover the hyperparameter space as well as possible.

Due to performance reasons we set the size to a low number (20). In a real world scenario, we would probably use 50 to 100.

```{r}

xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(), # needs to be a proportion
  finalize(mtry(), flight_train), 
  learn_rate(),
  size = 20 # usually we would set this to a higher number
)

xgb_grid

```

Notice that we had to treat `mtry()` differently because it depends on the actual number of predictors in the data.

```{r}
mtry() # biggest possible number of mtry is unknown
```

## Model workflow

Let's put the model specification into a workflow for convenience. Since we don't have any complicated data preprocessing, we can use `add_formula()` as our data preprocessor.

```{r}

xgb_wf <- workflow() %>%
  add_formula(arr_delay ~ .) %>%
  add_model(xgb_spec)

xgb_wf

```

## Cross-validation

Next, let's create cross-validation resamples for tuning our model. Usually we would set v to 10 but due to performance reasons, we choose 5.


```{r}
set.seed(123)

vb_folds <- vfold_cv(flight_train, 
                     v = 5,
                     strata = arr_delay)

vb_folds

```

## Hyperparamater tuning

We use `tune_grid()` with our tuneable workflow, our resamples, and our grid of parameters to try. Let's use `control_grid(save_pred = TRUE)` so we can explore the predictions afterwards.

```{r}

doParallel::registerDoParallel() # optimize processing

set.seed(234)

xgb_res <- tune_grid(
  xgb_wf,
  resamples = vb_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE) # so we can make a ROC curve 
)

xgb_res

```



# Explore results

## Metrics

We can explore the metrics for all these models.

```{r}

collect_metrics(xgb_res)

```

## ROC

We can also use visualization to understand our results. Reshape the data to make a graph.

```{r}
xgb_res_long <-  xgb_res %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) 

xgb_res_long %>% 
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")
```


## Show best parameters

What are the best performing sets of parameters?

```{r}
show_best(xgb_res, 
          "roc_auc")
```

## Select best parameters

There may have been lots of parameters, but we were able to get good performance with several different combinations. Let's choose the best one.

```{r}

best_auc <- select_best(xgb_res, 
                        "roc_auc")
best_auc

```


## Finalize workflow

Now let's finalize our tuneable workflow with these parameter values.

```{r}

final_xgb <- finalize_workflow(
  xgb_wf,
  best_auc
)

final_xgb

```

Instead of `tune()` placeholders, we now have real values for all the model hyperparameters.

Fit the final model on the training data:

```{r}

final_fit_xgb <- 
  final_xgb %>%
  fit(data = flight_train)

```


## Variable importance plot

To obtain information about the most important parameters in our model, we use a variable importance plot from the [vip package](https://koalaverse.github.io/vip/). 

```{r fig.width=7, fig.height=6}
library(vip)

final_fit_xgb %>%
  pull_workflow_fit() %>%
  vip(geom = "point") # variable importance plot

```

The predictors that are most important are shown at the top of the plot.


# Evaluate model 

It's time to go back to the testing set. Let's _evaluate_ our model one last time on the testing set. 


```{r}

final_res_pred <- 
  predict(final_fit_xgb, 
          flight_test, 
          type = "prob") %>% 
  bind_cols(flight_test %>% 
              select(arr_delay, 
                     time_hour, 
                     flight)) 

```


```{r}
final_res <- last_fit(final_xgb, split = flight_test)

collect_metrics(final_res)
```

Our results here indicate that we did not overfit during the tuning process. We can also create a ROC curve for the testing set.

## ROC

```{r, fig.width=6, fig.height=6}

final_res_pred %>%
   roc_curve(truth = arr_delay, 
            .pred_late) %>% 
  autoplot()
  
```


### AUC

Similarly, `roc_auc()` estimates the area under the curve:

```{r}

final_res_pred %>% 
  roc_auc(truth = arr_delay, .pred_late)

```


### Accuracy

We use the `metrics()` function to measure the performance of the model. It will automatically choose metrics appropriate for a given type of model. The function expects a tibble that contains the actual results (truth) and what the model predicted (estimate).

```{r}

final_res_pred %>%
  metrics(truth = arr_delay, 
          estimate = .pred_class)

```


### Recall 

```{r}

flights_fit_lr_mode %>%
  predict(test_data) %>%
    bind_cols(test_data) %>%
  recall(truth = arr_delay, 
          estimate = .pred_class)

```

### Precision

```{r}

flights_fit_lr_mode %>%
  predict(test_data) %>%
    bind_cols(test_data) %>%
  precision(truth = arr_delay, 
          estimate = .pred_class)

```

