---
title: "Tidymodels V: Tune Models"
subtitle: "Learn how to build models with tidymodels, Part 5"
author: "Prof. Dr. Jan Kirenz"
output:
 html_document:
  code_download: true 
  css: style.css 
  fig_height: 6
  fig_width: 8
  highlight: tango
  number_sections: yes
  theme: paper
  toc: yes
  toc_depth: 3
  toc_float: 
    collapsed: false
    smooth_scroll: true 
  df_print: paged
---


```{r setup, include=TRUE}
library(knitr)

knitr::opts_chunk$set(cache = TRUE, warning = FALSE, message = FALSE, 
                      echo = TRUE, fig.width = 8, fig.height = 5)
library(scales)
library(tidyverse)
library(tidymodels)
library(nycflights13)

```


This tutorial is based on Julia Silges excellent [demonstration](https://www.r-bloggers.com/2020/05/tune-xgboost-with-tidymodels-and-tidytuesday-beach-volleyball/) on how to use the tidymodels framework to tune XGBoost with tidymodels and the [tidymodels documentation](https://www.tidymodels.org/start/recipes/).*

# Explore the data

Our modeling goal is to predict whether a beach volleyball team of two won their match based on [game play stats like errors, blocks, attacks, etc from this week's #TidyTuesday dataset](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-19/readme.md) . 

This dataset is quite extensive so it's a great opportunity to try a more powerful machine learning algorithm like XGBoost. This model has lots of tuning parameters!

## Data import

```{r}
vb_matches <- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-05-19/vb_matches.csv', guess_max = 76000)

vb_matches
```

## Data transformation

This dataset has the match stats like serve errors, kills, and so forth divided out by the two players for each team, but we want those combined together because we are going to make a prediction **per team** (i.e. what makes a team more likely to win). Let's include predictors like gender, circuit, and year in our model along with the per-match statistics. Let's omit matches with `NA` values because we don't have all kinds of statistics measured for all matches.

```{r}
vb_parsed <- vb_matches %>%
  transmute(
    circuit,
    gender,
    year,
    w_attacks = w_p1_tot_attacks + w_p2_tot_attacks,
    w_kills = w_p1_tot_kills + w_p2_tot_kills,
    w_errors = w_p1_tot_errors + w_p2_tot_errors,
    w_aces = w_p1_tot_aces + w_p2_tot_aces,
    w_serve_errors = w_p1_tot_serve_errors + w_p2_tot_serve_errors,
    w_blocks = w_p1_tot_blocks + w_p2_tot_blocks,
    w_digs = w_p1_tot_digs + w_p2_tot_digs,
    l_attacks = l_p1_tot_attacks + l_p2_tot_attacks,
    l_kills = l_p1_tot_kills + l_p2_tot_kills,
    l_errors = l_p1_tot_errors + l_p2_tot_errors,
    l_aces = l_p1_tot_aces + l_p2_tot_aces,
    l_serve_errors = l_p1_tot_serve_errors + l_p2_tot_serve_errors,
    l_blocks = l_p1_tot_blocks + l_p2_tot_blocks,
    l_digs = l_p1_tot_digs + l_p2_tot_digs
  ) %>%
  na.omit()
```

Still plenty of data! Next, let's create separate dataframes for the winners and losers of each match, and then bind them together. I am using functions like `rename_with()` from [dplyr](https://www.tidyverse.org/blog/2020/05/dplyr-1-0-0-last-minute-additions/).

```{r}
winners <- vb_parsed %>%
  select(circuit, gender, year,
         w_attacks:w_digs) %>%
  rename_with(~ str_remove_all(., "w_"), w_attacks:w_digs) %>%
  mutate(win = "win")

losers <- vb_parsed %>%
  select(circuit, gender, year,
         l_attacks:l_digs) %>%
  rename_with(~ str_remove_all(., "l_"), l_attacks:l_digs) %>%
  mutate(win = "lose")

vb_df <- bind_rows(winners, losers) %>%
  mutate_if(is.character, factor)

```

Exploratory data analysis is always important before modeling. Let's make one plot to explore the relationships in this data.

## Boxplots

```{r}
vb_df %>%
  pivot_longer(attacks:digs, 
               names_to = "stat", 
               values_to = "value") %>%
  ggplot(aes(gender, value, 
             fill = win, 
             color = win)) +
  geom_boxplot(alpha = 0.4) +
  facet_wrap(~stat, 
             scales = "free_y", 
             nrow = 2) +
  labs(y = NULL, color = NULL, fill = NULL)
```

We can see differences in errors and blocks especially. There are lots more great examples of #TidyTuesday EDA out there to explore on [Twitter](https://twitter.com/hashtag/TidyTuesday)!

# Build a model

We can start by loading the tidymodels metapackage, and splitting our data into training and testing sets.

## Data splitting

```{r}
library(tidymodels)

set.seed(123)
vb_split <- initial_split(vb_df, strata = win)

#vb_split$id <- tibble(id = "Resample1")

vb_train <- training(vb_split)
vb_test <- testing(vb_split)

```

## XGBoost hyperparamter

An XGBoost model is based on trees, so we don't need to do much preprocessing for our data; we don't need to worry about the factors or centering or scaling our data. Let's just go straight to setting up our model specification. Sounds great, right? On the other hand, we are going to tune **a lot** of model hyperparameters.


```{r}

# ?boost_tree

xgb_spec <- boost_tree(
  trees = 10, # usually we woul set this to 1000
  tree_depth = tune(), # tune placeholder
  min_n = tune(), 
  loss_reduction = tune(),  ## first three: model complexity
  sample_size = tune(), 
  mtry = tune(),  ## randomness
  learn_rate = tune(),    ## step size
) %>% 
  set_engine("xgboost") %>% 
  set_mode("classification")

xgb_spec

```


* mtry: The number of predictors that will be randomly sampled at each split when creating the tree models.

* trees: The number of trees contained in the ensemble.

* min_n: The minimum number of data points in a node that are required for the node to be split further.

* tree_depth: The maximum depth of the tree (i.e. number of splits).

* learn_rate: The rate at which the boosting algorithm adapts from iteration-to-iteration.

* loss_reduction: The reduction in the loss function required to split further.

* sample_size: The amount of data exposed to the fitting routine.

* stop_iter: The number of iterations without improvement before stopping.

## Grid space

Let's set up possible values for these hyperparameters to try. Let's use a space-filling design so we can cover the hyperparameter space as well as possible.

Option 1: (would take a long time)

```{r}
# ?grid_regular
```

Option 2: Space filling design (more efficient). 

```{r}
#?grid_latin_hypercube
```

Due to performance reasons we set the size to a very low number (3). In a real world scenario, we would probably use 50 to 100.

```{r}

xgb_grid <- grid_latin_hypercube(
  tree_depth(),
  min_n(),
  loss_reduction(),
  sample_size = sample_prop(), # needs to be a proportion
  finalize(mtry(), vb_train), 
  learn_rate(),
  size = 3 # usually we would set this to a higher number
)

xgb_grid
```

Notice that we had to treat `mtry()` differently because it depends on the actual number of predictors in the data.

```{r}
mtry() # biggest possible number of mtry is unknown
```

## Model workflow

Let's put the model specification into a workflow for convenience. Since we don't have any complicated data preprocessing, we can use `add_formula()` as our data preprocessor.

```{r}
xgb_wf <- workflow() %>%
  add_formula(win ~ .) %>%
  add_model(xgb_spec)

xgb_wf
```

## Cross-validation

Next, let's create cross-validation resamples for tuning our model. Usually we would set v to 10 but due to performance reasons, we choose 5.


```{r}
set.seed(123)

vb_folds <- vfold_cv(vb_train, 
                     v = 5,
                     strata = win)

vb_folds

```

## Hyperparamater tuning

We use `tune_grid()` with our tuneable workflow, our resamples, and our grid of parameters to try. Let's use `control_grid(save_pred = TRUE)` so we can explore the predictions afterwards.

```{r}

doParallel::registerDoParallel() # optimize processing

set.seed(234)

xgb_res <- tune_grid(
  xgb_wf,
  resamples = vb_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE) # so we can make a ROC curve 
)

xgb_res
```

This could take a a while to finish.

# Explore results

## Metrics

We can explore the metrics for all these models.

```{r}
collect_metrics(xgb_res)
```

## ROC

We can also use visualization to understand our results. Reshape the data to make a graph.

```{r}
xgb_res_long <-  xgb_res %>% 
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  select(mean, mtry:sample_size) %>%
  pivot_longer(mtry:sample_size,
               values_to = "value",
               names_to = "parameter"
  ) 

xgb_res_long %>% 
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "AUC")
```

Remember that we used a space-filling design for the parameters to try. It looks like higher values for tree depth were better, but other than that, the main thing I take away from this plot is that there are several combinations of parameters that perform well.

## Show best parameters

What are the best performing sets of parameters?

```{r}
show_best(xgb_res, 
          "roc_auc")
```

## Select best parameters

There may have been lots of parameters, but we were able to get good performance with several different combinations. Let's choose the best one.

```{r}

best_auc <- select_best(xgb_res, 
                        "roc_auc")
best_auc

```



## Finalize workflow

Now let's finalize our tuneable workflow with these parameter values.

```{r}

final_xgb <- finalize_workflow(
  xgb_wf,
  best_auc
)

final_xgb

```


Instead of `tune()` placeholders, we now have real values for all the model hyperparameters.

## Variable importance plot

What are the most important parameters for [variable importance](https://koalaverse.github.io/vip/)?

```{r fig.width=7, fig.height=6}
library(vip)

final_xgb %>%
  fit(data = vb_train) %>%
  pull_workflow_fit() %>%
  vip(geom = "point") # variable importance plot

```

The predictors that are most important in a team winning vs. losing their match are the number of kills, errors, and attacks. There is almost no difference between the two circuits, and very little difference by gender.



# Evaluate model 

## Last fit

It's time to go back to the testing set. Let's use `last_fit()` to _fit_ our model one last time on the training data and _evaluate_ our model one last time on the testing set. Notice that this is the first time we have used the testing data during this whole modeling analysis.

```{r}
final_res <- last_fit(final_xgb, split = vb_test)

collect_metrics(final_res)
```

Our results here indicate that we did not overfit during the tuning process. We can also create a ROC curve for the testing set.



## ROC

```{r, fig.width=6, fig.height=6}

final_res_pred <- 
  final_res %>%
  collect_predictions() %>%
  roc_curve(win, .pred_win)
  
final_res_pred %>% 
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_line(size = 1.5, color = "midnightblue") +
  geom_abline(
    lty = 2, alpha = 0.5,
    color = "gray50",
    size = 1.2
  )
```
