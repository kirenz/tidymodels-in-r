---
title: "Tidymodels III: Build Models with Recipes"
subtitle: "Learn how to build models with tidymodels, Part 3"
author: "Prof. Dr. Jan Kirenz"
output:
 html_document:
  code_download: true 
  css: style.css 
  fig_height: 6
  fig_width: 8
  highlight: tango
  number_sections: yes
  theme: paper
  toc: yes
  toc_depth: 3
  toc_float: 
    collapsed: false
    smooth_scroll: true 
  df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidyverse)
library(tidymodels)
```

*The following content is based on the [tidymodels documentation](https://www.tidymodels.org/start/recipes/) and Alisson Hill's [tidymodels workshop](https://alison.rbind.io/tags/tidymodels/).*

In this tutorial, we’ll explore a tidymodels package, `recipes`, which is designed to help you preprocess your data before training your model. 

Recipes are built as a series of preprocessing steps, such as:

* converting qualitative predictors to indicator variables (also known as dummy variables),
* transforming data to be on a different scale (e.g., taking the logarithm of a variable),
* transforming whole groups of predictors together,
* extracting key features from raw variables (e.g., getting the day of the week out of a date variable),

and so on. If you are familiar with R’s formula interface, a lot of this might sound familiar and like what a formula already does. Recipes can be used to do many of the same things, but they have a much wider range of possibilities. This article shows how to use recipes for modeling.

In summary, the idea of the [recipes package](https://recipes.tidymodels.org) is to define a recipe or blueprint that can be used to sequentially define the encodings and preprocessing of the data (i.e. “feature engineering”) before we build our models.

# Data preparation

Import data and split the data into training and testing sets using `initial_split()`

```{r}
library(tidyverse)
library(tidymodels)

ames <- read_csv("https://raw.githubusercontent.com/kirenz/datasets/master/ames.csv")

ames <- ames %>%
 select(-matches("Qu"))

set.seed(100)

new_split <- initial_split(ames) 
new_train <- training(new_split) 
new_test <- testing(new_split)

```

# Recipe

Next, we use a `recipe()` to build a set of steps for data preprocessing and feature engineering.

* First, we must tell the `recipe()` what our model is going to be (using a formula here) and what our training data is.
* `step_novel()` will convert all nominal variables to factors.
* We then convert the factor columns into (one or more) numeric binary (0 and 1) variables for the levels of the training data.
* We remove any numeric variables that have zero variance.
* We normalize (center and scale) the numeric variables. 

## recipe()

```{r}

ames_rec <-
  recipe(Sale_Price ~ ., data = new_train) %>%
  step_novel(all_nominal(), -all_outcomes()) %>%
  step_dummy(all_nominal()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())

# Show the content of our recipe
ames_rec
  
```

## prep()

Finally, we `prep()` the `recipe()`. This means we actually do something with the steps and our training data.

```{r}

ames_prep <- prep(ames_rec)

```


Print a summary of our prepped recipe:

```{r}

summary(ames_prep)

```

## juice()

To obtain the Dataframe from the prepped recipe, we use the function `juice()`. When we `juice()` the recipe, we squeeze that training data back out, transformed in the ways we specified:

```{r}

juice(ames_prep)

```

The prepped recipe `ames_prep` contains all our transformations for data preprocessing and feature engineering, *as well as* the data these transformations were estimated from. 

## bake()

We now can simply apply all of the recipe transformations to the testing data. The function to perform this is called `bake()`:


```{r}

test_trans <- bake(ames_prep, new_data = new_test)

```


Now it's time to **specify** and then **fit** our models. 

# Model building

## Specify model

```{r}

lm_spec <- 
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode(mode = "regression")

```

## Fit model

```{r}

lm_fit <- 
  lm_spec %>%
  fit(Sale_Price ~ . , data = juice(ames_prep))

```


## Evaluate model

```{r}

set.seed(100)

cv_folds <-
 vfold_cv(juice(ames_prep), 
          v = 10, 
          strata = Sale_Price,
          breaks = 4) 

lm_res <-
  lm_spec %>% 
  fit_resamples(
    Sale_Price ~ .,
    resamples = cv_folds
    )

lm_res %>% 
  collect_metrics()

```


## Evaluate final model

Finally, let's use our testing data and see how we can expect this model to perform on new data.

```{r}

lm_fit %>% 
 predict(test_trans) %>%
 mutate(truth = test_trans$Sale_Price) %>%
 rmse(truth, .pred)

```


# Components of our recipe

Let's have a closer look at the different components of the recipe.

## recipe()

First of all, we created a simple recipe (we call it `rec`) containing only an outcome (`Sale_Price`) and predictors (all other variables in the dataset: `.`). To demonstrate the use of recipes step by step, we create a new object with the name `rec`:

```{r}

rec <- recipe(Sale_Price ~ ., data = ames)

```

The formula `Sale_Price ~ .` indicates outcomes vs predictors.

## Helper functions

Here some helper functions for selecting sets of variables:

* `all_predictors()`: Each x variable (right side of ~)
* `all_outcomes()`: Each y variable (left side of ~)
* `all_numeric()`: Each numeric variable
* `all_nominal()`: Each categorical variable (e.g. factor, string)
* `dplyr::select()` helpers starts_with('Lot_'), etc.


## step_novel()

[`step_novel()`](https://recipes.tidymodels.org/reference/step_novel.html) will convert all nominal variables to factors. It adds a catch-all level to a factor for any new values, which lets R intelligently predict new levels in the test set. Missing values will remain missing.

```{r}

rec %>%
  step_novel(all_nominal(), -all_outcomes())

```

## step_dummy()

Converts nominal data into dummy variables.

```{r}

rec %>%
 step_dummy(all_nominal())

```


## step_zv()

`step_zv()` removes zero variance variables (variables that contain only a single value). 

```{r}

rec %>%
  step_zv(all_predictors())

```

When the recipe is applied to the data set, a column could contain only zeros. This is a "*zero-variance predictor*" that has no information within the column. While some R functions will not produce an error for such predictors, it usually causes warnings and other issues. `step_zv()` will remove columns from the data when the training set data have a single value- This step should be added to the recipe after `step_dummy()`.

## step_normalize()

Centers then scales numeric variable (mean = 0, sd = 1)

```{r}

rec %>%
  step_normalize(all_numeric())

```


# Workflows

To combine the data preparation with the model building, we could use the package [workflows](https://workflows.tidymodels.org). A workflow is an object that can bundle together your pre-processing, modeling, and post-processing requests:

## workflow()

```{r}

new_wf <-
 workflow() %>%
 add_recipe(ames_rec) %>%
 add_model(lm_spec)

```

## fit()


```{r}

new_wf_fit <- 
  new_wf %>% 
  fit(data = new_train)

```

## predict() & rmse()

Make predictions and calculate the RMSE for our test data. 


```{r}
new_wf_fit %>% 
 predict(new_test) %>%
 mutate(truth = new_test$Sale_Price) %>%
 rmse(truth, .pred)

```


